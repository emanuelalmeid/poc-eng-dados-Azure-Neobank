# Poc-eng-dados-Azure-Neobank
## Overview
Welcome!! üöÄ

Esse projeto se baseia na tend√™ncia onde cada vez mais √© necess√°rio riqueza, qualidade, escalabilidade e seguran√ßa dos dados para gerar insights e valor ao neg√≥cio. Dessa forma, √© proposta uma infraestrutura utilizando a cloud Azure para coletar, processar e armazenar os dados de forma escal√°vel.

Para isso foi uma proposto uma arquitetura utilizando Data lake storage Gen 2 com tr√™s camadas sendo a bronze respons√°vel pelo armazenamento dos dados originais em formato CSV, a silver armazenando os dados e utilizando o slow changing dimension tipo 2 para realizar a historiza√ß√£o e por fim a camada ouro que estar√° dispon√≠vel para armazenar dados mais estruturados para an√°lise, ser√° utilizada para remo√ß√£o de dados sens√≠veis que n√£o podem ser disponibilizados diretamente para os analistas.
Ainda sobre a arquitetura, o projeto conta com o Azure databricks respons√°vel para realizar as transforma√ß√µes em cada camadas do data lake, Azure Factory para ingest√£o dos dados, orquestra√ß√£o e ativa√ß√£o dos notebooks de forma peri√≥dica e o Azure Synapse Analytics com a fun√ß√£o de copiar e armazenar os dados da camada ouro e disponibiliz√°-los para a ferramenta de dataviz que ser√° o PowerBi.


<img src="/Imagens/azure-pipeline-schematic.drawio.png">


### Contexto do neg√≥cio
NeoBank √© um banco que est√° disparando seu crescimento. Visto isso, a empresa est√° investindo em testes relacionados a cloud para crescer sua infraestrutura dentro da nuvem gerando solu√ß√µes que escalem de acordo com a velocidade do seu crescimento. Visto isso, foi direcionado para o time de engenharia de dados, realizar uma poc (prova de conceito) disponibilizando dados relacionados a clientes do banco para que o time de an√°lise de dados possa criar relat√≥rios e extrair informa√ß√µes valiosas para o neg√≥cio.
### Base de dados
A base √© um arquivo .csv anexado neste reposit√≥rio onde possui 16 colunas e 10000 linhas de dados relacionados aos clientes do banco Neobank. Esta poc inclui a ingest√£o, transforma√ß√£o e carregamento desta dimens√£o.  

### Grupo de recursos
Para come√ßar o desenvolvimento do projeto, foi criado um grupo de recursos destinado a manter os recursos que comp√µem a solu√ß√£o. Esse recurso tamb√©m traz o benef√≠cio de monitorar todos os custos relacionados ao projeto.

<img src="/Imagens/resourcegroup.PNG">

### Storage account
Por meio da cria√ß√£o de containers foram criados as tr√™s camadas do data lake em modo privado:

<img src="/Imagens/containers.PNG">

## ETL

### Ingest√£o 
Para extra√ß√£o dos dados e ingest√£o no data lake foi criado a partir da atividade Copy do Azure Datafactory onde o Source foi definido como uma fonte HTTP o qual est√° relacionada ao link do arquivo csv disponibilizado neste mesmo reposit√≥rio e o Source do fluxo conectado a camada bronze do datalake gerando um arquivo com o mesmo nome da fonte. Para realizar esse processo, foi necess√°rio a cria√ß√£o de um link de servi√ßo conectando a storage account ao data factory.


<img src="/Imagens/copy.PNG">

### Transforma√ß√£o
Ap√≥s a ingest√£o os dados estar√£o dispon√≠veis para transforma√ß√£o, foi utilizado Azure databricks se conectando ao data lake para transforma√ß√£o dos dados gerando tr√™s scripts.

<b>1.</b>  Azure (ADD): Montar conex√£o entre azure databricks e containers.

<b>2.</b>  Bronze to Silver: Slow change dimension tipo 2.

<b>3.</b>  Silver to Gold: Remo√ß√£o de dados sens√≠veis.

Imagem abaixo ilustra as transforma√ß√µes aplicadas no script 2 e 3

<img src="/Imagens/Scripts.png">

### Carregamento 
A partir do Azure Synapse Analytics foi criado um bando de dados sem servidor. A partir dele foi criado uma tabela externa que reflete o conte√∫do do arquivo parquet encontrado na camada gold.

<img src="/Imagens/TABLEXTERNA.PNG">

Para cria√ß√£o da tabela externa e visualiza√ß√£o dos dados com seguran√ßa, foram criados 3 scripts:

<b>1.</b>  Master Key

<b>2.</b>  CREDENCIAL

<b>3.</b>  external table

<img src="/Imagens/SCRIPTSASA.PNG">


### Orquestra√ß√£o 
Utilizando o Data Factory foi criado a√ß√µes para os dois notebooks respons√°veis pela transforma√ß√£o dos dados orquestrando cada a√ß√£o e criando a pipeline respons√°vel extrair, transformar e disponibilizar os dados.  

<img src="/Imagens/pipelineADF.PNG">



## Dashboard
Para validar a pipeline e garantir que os dados estejam chegando ao seu destino final, foi a partir do banco de dados importado os dados para o Power Bi desktop. Dessa forma, foi elaborado um simples dashboard para realizar a visualiza√ß√£o dos dados da poc.

<img src="/Imagens/Power BI.PNG">
  



## Informa√ß√µes da autor
<li>Nome: Emanuel de Almeida Alves</li>
<li>E-mail: emanuelalmeidaalves123@gmail.com</li>
<li>Linkedin: https://www.linkedin.com/in/emanueldealmeida/</li>
